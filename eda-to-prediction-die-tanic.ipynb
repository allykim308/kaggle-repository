{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-14T05:15:58.133761Z","iopub.execute_input":"2022-07-14T05:15:58.134326Z","iopub.status.idle":"2022-07-14T05:15:58.159653Z","shell.execute_reply.started":"2022-07-14T05:15:58.134283Z","shell.execute_reply":"2022-07-14T05:15:58.158524Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Contents of the Notebook:\n\npart1: Exploratory Data Analysis(EDA):\n1) Analysis of the features\n2) Finding any relations or trends considering multiple features.\n\nPart2:Feature Engineering and Data Cleaning:\n1) Adding any few features\n2) Removing redundant features\n3) Converting features into suitbale form for modeling\n\nPart3: Predictive Modeling\n1)Running Basic Algorithms\n2) Coass Validation\n3) Ensembling\n4) Important Features Extraction\n\nPart 1: Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import numpy as np # numpy는 수학 및 과학 연산을 위한 패키지\nimport pandas as pd #pandas는 dataframe을 주로 다루기 위한 라이브러리\nimport matplotlib.pyplot as plt #matplotlib은 그 안의 각각의 함수를 사용해서 간편하게 그래프를 만들고 변화를 주기 위한 라이브러\nimport seaborn as sns #Seaborn은 Matplotlib을 기반으로 다양한 색상 테마와 통계용 차트 등의 기능을 추가한 시각화 패키지\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.201819Z","iopub.execute_input":"2022-07-14T05:15:58.202450Z","iopub.status.idle":"2022-07-14T05:15:58.378891Z","shell.execute_reply.started":"2022-07-14T05:15:58.202418Z","shell.execute_reply":"2022-07-14T05:15:58.377671Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/titanic-machine-learning-from-disaster/train.csv') #reading in the file and store in data","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.380771Z","iopub.execute_input":"2022-07-14T05:15:58.381134Z","iopub.status.idle":"2022-07-14T05:15:58.400429Z","shell.execute_reply.started":"2022-07-14T05:15:58.381070Z","shell.execute_reply":"2022-07-14T05:15:58.399283Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.head() #showing first five data sets","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.401955Z","iopub.execute_input":"2022-07-14T05:15:58.403019Z","iopub.status.idle":"2022-07-14T05:15:58.429861Z","shell.execute_reply.started":"2022-07-14T05:15:58.402972Z","shell.execute_reply":"2022-07-14T05:15:58.428591Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum() #checking for total null values","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.431998Z","iopub.execute_input":"2022-07-14T05:15:58.432362Z","iopub.status.idle":"2022-07-14T05:15:58.441839Z","shell.execute_reply.started":"2022-07-14T05:15:58.432330Z","shell.execute_reply":"2022-07-14T05:15:58.440816Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The age, Cabin and Embarked have null values, I will try to fix them.\n\n# How many survived? ","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize = (18, 8)) # creating 1*2 axis and access them trhough returned array ax\n#matplotlib.pyplot 모듈의 subplot() 함수는 여러 개의 그래프를 하나의 그림에 나타내도록 합니다.\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.1f%%', ax = ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\n#The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\nsns.countplot('Survived', data=data,ax=ax[1]) #ax[1]은 오른쪽에 있는 subplot, \nax[1].set_title('Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.475778Z","iopub.execute_input":"2022-07-14T05:15:58.476442Z","iopub.status.idle":"2022-07-14T05:15:58.770445Z","shell.execute_reply.started":"2022-07-14T05:15:58.476399Z","shell.execute_reply":"2022-07-14T05:15:58.769156Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"It is evident that not many passengers survived the accident. \nOut of 891 passengers in training set, only around 350 survived i.e. Only **38.4**% of the totoal training set survived the crash. We need to dig down more to get better insights from the data and see which categories for the passengers did survive and who didn't.\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being sex, port of ambarcation, age, etc.\n\nFirst let us understand the different type of features.\n사고에서 살아남은 승객은 많지 않은 것이 분명합니다.\n훈련 세트에 있는 891명의 승객 중 약 350명만 생존했습니다. 즉, 총 훈련 세트의 **38.4**%만이 충돌에서 생존했습니다. 데이터에서 더 나은 통찰력을 얻고 승객의 어떤 범주가 생존하고 누가 생존하지 않았는지 확인하기 위해 더 파고들 필요가 있습니다.\n데이터 세트의 다양한 기능을 사용하여 생존율을 확인하려고 합니다. 일부 기능은 성별, 철수항, 연령 등입니다.\n\n먼저 다양한 유형의 기능을 이해하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"# Type of Features\n\n# Categorical Features:\nA categorical variable is one that has two of more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories(male and female). Now we cannot sort of give any ordering to such variables. They are also known as **Nominal Variables**.\n(nominal variable is a type of variable that is used to name, label or categorize particular attributes that are being measured.)\n\nCategorical Features in the dataset: Sex, Embarked.\n\nOrdinal Features:\nAn ordinal variable is similar to categorical values but the differnce between them is that we can have relative ordering or sorting between the values. For eg: if we have a feature like Height with values tall, mediunm, short, then height is a ordinal vairable. Here we can have a relative sort in the variable.\n\nOrdinal Features in the dataset:PClass\n\n# Continuous Feature:\na feature is said to be continuous if it can take values between any two points or between the minimum ro maximum values in the features column.\nContinuos Features in teh dataset:Age\n\nAnalysing The Features\nSex--> Categorical Feature","metadata":{}},{"cell_type":"code","source":"data.groupby(['Sex', 'Survived'])['Survived'].count() # 성별을 기준으로 하고 다시 생존여부 기준으로 분류하여 각 생존자 숫자를 표기","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.772356Z","iopub.execute_input":"2022-07-14T05:15:58.772669Z","iopub.status.idle":"2022-07-14T05:15:58.783789Z","shell.execute_reply.started":"2022-07-14T05:15:58.772643Z","shell.execute_reply":"2022-07-14T05:15:58.782398Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax = ax[0])\nax[0].set_title('Sex: Survived vs Dead')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:58.785230Z","iopub.execute_input":"2022-07-14T05:15:58.785935Z","iopub.status.idle":"2022-07-14T05:15:59.054571Z","shell.execute_reply.started":"2022-07-14T05:15:58.785887Z","shell.execute_reply":"2022-07-14T05:15:59.053487Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"This looks interesting. THe number of men on the ship is a lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18~19%.\nPclass --> Ordinal Feature","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived, margins = True).style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.057950Z","iopub.execute_input":"2022-07-14T05:15:59.058629Z","iopub.status.idle":"2022-07-14T05:15:59.159418Z","shell.execute_reply.started":"2022-07-14T05:15:59.058582Z","shell.execute_reply":"2022-07-14T05:15:59.158565Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0]) # plot은 여러 그래프를 그리는데 쓰임\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.160529Z","iopub.execute_input":"2022-07-14T05:15:59.160992Z","iopub.status.idle":"2022-07-14T05:15:59.423218Z","shell.execute_reply.started":"2022-07-14T05:15:59.160965Z","shell.execute_reply":"2022-07-14T05:15:59.422220Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"People say 'Moeny can't buy Everything.' But we can clearly see that Passengers of P class 1 were given a very high priorty while rescue. Even though the number of passengers in pclass 3 were a lot higher still the number of survival from them is very low, somewhere around 25%. For pclass1, survived around 63% while for pclass 2 is around 48%. So money and status matters. Such a materialistic world.\nLet's dive in little bit more and check for other interesting observations. Let's check survival rate with Sex and pclass Togther. ","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins = True).style.background_gradient(cmap = 'summer_r')#총합을 보고싶다면 margins = true\n#pandas.crosstab(index(row), columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.424749Z","iopub.execute_input":"2022-07-14T05:15:59.425187Z","iopub.status.idle":"2022-07-14T05:15:59.478245Z","shell.execute_reply.started":"2022-07-14T05:15:59.425144Z","shell.execute_reply":"2022-07-14T05:15:59.477126Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Pclass','Survived', hue='Sex', data=data)\nplt.show()\n\n#seaborn.factorplot() method is used to draw a categorical plot onto a FacetGrid.\n#x, y : This parameter take names of variables in data, Inputs for plotting long-form data.\n#hue : (optional)This parameter take column name for colour encoding\n#data : This parameter take DataFrame, Long-form (tidy) dataset for plotting. Each column should correspond to a variable, and each row should correspond to an observation.","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.479772Z","iopub.execute_input":"2022-07-14T05:15:59.480118Z","iopub.status.idle":"2022-07-14T05:15:59.921267Z","shell.execute_reply.started":"2022-07-14T05:15:59.480066Z","shell.execute_reply":"2022-07-14T05:15:59.919957Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We use factorplot in this case, because they make the seperation of catergorical values easy.\nLooking at the Crosstab adn the factorplot, we can easily infer that survival for women from plcass 1 is about 95~96% as only 3 out 94 women from class 1 died.\nIt is evident taht irrespective of pcalss, women were given first priorty while rescue. Even men from pclass 1 have a very low survival rate. \nLooks like pclass is also an important feature. Lets analyse other features.\n\n이 경우 factorplot을 사용합니다. 범주형 값을 쉽게 분리할 수 있기 때문입니다. Crosstab 및 factorplot을 보면 class 1의 여성 94명 중 3명만 사망했기 때문에 plcass 1의 여성 생존율이 약 95~96%임을 쉽게 유추할 수 있습니다. Pclass에 관계없이 여성이 구조되는 동안 우선적으로 주어졌다는 것은 분명합니다. pclass 1의 남성조차도 생존율이 매우 낮습니다. pclass도 중요한 기능인 것 같습니다. 다른 기능을 분석할 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"# # # Age--> Continuous Feature ","metadata":{}},{"cell_type":"code","source":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.924560Z","iopub.execute_input":"2022-07-14T05:15:59.924888Z","iopub.status.idle":"2022-07-14T05:15:59.933060Z","shell.execute_reply.started":"2022-07-14T05:15:59.924859Z","shell.execute_reply":"2022-07-14T05:15:59.931936Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8)) # nrows=1, ncols=2\n#matplotlib.pyplot 모듈의 subplot() 함수는 여러 개의 그래프를 하나의 그림에 나타내도록 합니다.\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1]) \n#seaborn.violinplot(x, y, hue, data,)\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:15:59.935961Z","iopub.execute_input":"2022-07-14T05:15:59.936624Z","iopub.status.idle":"2022-07-14T05:16:00.351035Z","shell.execute_reply.started":"2022-07-14T05:15:59.936589Z","shell.execute_reply":"2022-07-14T05:16:00.350200Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Observations:\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.\n\n3)For males, the survival chances decreases with an increase in age.\n\nAs we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie??\n\nBingo!!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups.\n\n''What's In A Name??''---> Feature :p","metadata":{}},{"cell_type":"code","source":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')#([A-Za-z]+)\\. 점앞에 붙 있는 모든 문자\n\n#[a-zA-Z] : 알파벳 모두","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.352160Z","iopub.execute_input":"2022-07-14T05:16:00.352667Z","iopub.status.idle":"2022-07-14T05:16:00.390683Z","shell.execute_reply.started":"2022-07-14T05:16:00.352637Z","shell.execute_reply":"2022-07-14T05:16:00.389466Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Oksay so here we are using the Regex:[A-za-z]+0 So what it does is, it looks for strings which lie between A-Z or a-z and followed by a .(dot). So we successfully extract the initials from the name.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.392945Z","iopub.execute_input":"2022-07-14T05:16:00.393338Z","iopub.status.idle":"2022-07-14T05:16:00.441939Z","shell.execute_reply.started":"2022-07-14T05:16:00.393304Z","shell.execute_reply":"2022-07-14T05:16:00.441115Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Okay so there are some misspelled Initials like Mlle or Mme that stands for Miss. I will replace them with Miss and same thing for other values.\nMlle는 마드모아젤의 약자 즉 아가씨, Mme는 마담의 약자, 즉 부인.","metadata":{}},{"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.443232Z","iopub.execute_input":"2022-07-14T05:16:00.443779Z","iopub.status.idle":"2022-07-14T05:16:00.452497Z","shell.execute_reply.started":"2022-07-14T05:16:00.443749Z","shell.execute_reply":"2022-07-14T05:16:00.451149Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.groupby('Initial')['Age'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.453978Z","iopub.execute_input":"2022-07-14T05:16:00.454323Z","iopub.status.idle":"2022-07-14T05:16:00.471041Z","shell.execute_reply.started":"2022-07-14T05:16:00.454293Z","shell.execute_reply":"2022-07-14T05:16:00.469914Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Filling NaN Ages","metadata":{}},{"cell_type":"code","source":"## Assigning the NaN Values with the Ceil values of the mean ages\n#소숫점으로 나온 값을 올림한\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.472808Z","iopub.execute_input":"2022-07-14T05:16:00.473639Z","iopub.status.idle":"2022-07-14T05:16:00.487702Z","shell.execute_reply.started":"2022-07-14T05:16:00.473593Z","shell.execute_reply":"2022-07-14T05:16:00.486616Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data.Age.isnull().any() #so no null values left finally","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.489280Z","iopub.execute_input":"2022-07-14T05:16:00.490269Z","iopub.status.idle":"2022-07-14T05:16:00.497393Z","shell.execute_reply.started":"2022-07-14T05:16:00.490222Z","shell.execute_reply":"2022-07-14T05:16:00.496166Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived=0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\nax[1].set_title('Survived=1')\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1], bins=20, edgecolor='black', color='green')\nax[1].set_title('Survived=1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.498916Z","iopub.execute_input":"2022-07-14T05:16:00.499625Z","iopub.status.idle":"2022-07-14T05:16:00.891011Z","shell.execute_reply.started":"2022-07-14T05:16:00.499580Z","shell.execute_reply":"2022-07-14T05:16:00.890159Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1) The Toddlers(age<5) were saved in large numbers(woman and children first policy)\n2) The oldest passenger was saved(80yrs)\n3) Maximum number of death was in the age group of 30~40.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass','Survived',col='Initial',data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:00.892150Z","iopub.execute_input":"2022-07-14T05:16:00.892647Z","iopub.status.idle":"2022-07-14T05:16:02.024605Z","shell.execute_reply.started":"2022-07-14T05:16:00.892613Z","shell.execute_reply":"2022-07-14T05:16:02.023485Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"sns.pointplot('Pclass', 'Survived', col='Initial', data=data) #factor plot과 pointplot의 차이점. \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:02.026115Z","iopub.execute_input":"2022-07-14T05:16:02.027280Z","iopub.status.idle":"2022-07-14T05:16:02.266323Z","shell.execute_reply.started":"2022-07-14T05:16:02.027232Z","shell.execute_reply":"2022-07-14T05:16:02.265321Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"The women and children first policy thus holds true irrespective of the class.","metadata":{}},{"cell_type":"markdown","source":"# Embarked --> Categorical Value","metadata":{}},{"cell_type":"code","source":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins=True).style.background_gradient(cmap='PuRd')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:02.267847Z","iopub.execute_input":"2022-07-14T05:16:02.268189Z","iopub.status.idle":"2022-07-14T05:16:02.340451Z","shell.execute_reply.started":"2022-07-14T05:16:02.268161Z","shell.execute_reply":"2022-07-14T05:16:02.339249Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Chances for Survival by Port of Embakation","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Embarked','Survived', data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:02.342431Z","iopub.execute_input":"2022-07-14T05:16:02.342987Z","iopub.status.idle":"2022-07-14T05:16:02.648576Z","shell.execute_reply.started":"2022-07-14T05:16:02.342942Z","shell.execute_reply":"2022-07-14T05:16:02.647441Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The chances for survival for Port C is highest around 0.55 while it is lowest for S.","metadata":{}},{"cell_type":"code","source":"f,ax = plt.subplots(2,2, figsize=(20, 15))\nsns.countplot('Embarked', data=data,ax=ax[0,0])\nax[0,0].set_title('No. of Passnegers Boarded')\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=data, ax = ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:02.649738Z","iopub.execute_input":"2022-07-14T05:16:02.650362Z","iopub.status.idle":"2022-07-14T05:16:03.209086Z","shell.execute_reply.started":"2022-07-14T05:16:02.650327Z","shell.execute_reply":"2022-07-14T05:16:03.207919Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Observation\n1) Maximum passengers boarded from S. Majority of them being from Pclass3.\n2) The passengers from C look to be lucky as a good proportion of them survived.\n3) The Embark S looks to be the port from where majortiy of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass around 81% didn't survive.\n4) Port Q had almost 95% if the passengers from Pcalss3.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', col = 'Embarked', data=data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:03.214332Z","iopub.execute_input":"2022-07-14T05:16:03.214652Z","iopub.status.idle":"2022-07-14T05:16:04.247318Z","shell.execute_reply.started":"2022-07-14T05:16:03.214624Z","shell.execute_reply":"2022-07-14T05:16:04.246021Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1) The survival chances are almost 1 for women for Plcass1 and Pcalss 2 irrespective of the Pclass.\n\n2) Port S looks to be very unlucky for Pclass3 passengers as the survival rate for both men women is very low.(money matters)\n\n3) Port Q looks to be unluckiest for Men as almost all were from Pclass 3.","metadata":{}},{"cell_type":"markdown","source":"# Filling Embarked Nan\nAs we saw that maximum passengers boarded from port S, we replace NaN with S.","metadata":{}},{"cell_type":"code","source":"data['Embarked'].fillna('S', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.248895Z","iopub.execute_input":"2022-07-14T05:16:04.249260Z","iopub.status.idle":"2022-07-14T05:16:04.255058Z","shell.execute_reply.started":"2022-07-14T05:16:04.249230Z","shell.execute_reply":"2022-07-14T05:16:04.253844Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"data.Embarked.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.256562Z","iopub.execute_input":"2022-07-14T05:16:04.256933Z","iopub.status.idle":"2022-07-14T05:16:04.269803Z","shell.execute_reply.started":"2022-07-14T05:16:04.256903Z","shell.execute_reply":"2022-07-14T05:16:04.268557Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Survived).style.background_gradient(cmap='summer_r') #교차 테이블. (index, col)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.271599Z","iopub.execute_input":"2022-07-14T05:16:04.272016Z","iopub.status.idle":"2022-07-14T05:16:04.300161Z","shell.execute_reply.started":"2022-07-14T05:16:04.271981Z","shell.execute_reply":"2022-07-14T05:16:04.299336Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=data,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.pointplot('SibSp','Survived',data=data,ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.301447Z","iopub.execute_input":"2022-07-14T05:16:04.301753Z","iopub.status.idle":"2022-07-14T05:16:04.959323Z","shell.execute_reply.started":"2022-07-14T05:16:04.301726Z","shell.execute_reply":"2022-07-14T05:16:04.958123Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r')\n#pd.crosstab(data.SibSp, data.Pclass) 으로 치면 흑백으로 출력됨, 즉 .style 이하는 색상 관련 ","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.961098Z","iopub.execute_input":"2022-07-14T05:16:04.961803Z","iopub.status.idle":"2022-07-14T05:16:04.990483Z","shell.execute_reply.started":"2022-07-14T05:16:04.961757Z","shell.execute_reply":"2022-07-14T05:16:04.989291Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Observations:\nThe barplot and factorplot shows that if a passenger is alone onboard with no siblings, he has 34.5% survivial rate. The graph roughly decreases if the number of sibling increases. This makes sense. That is if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is 0%. The reason maybe Pclass?\n\nThe reason is Pcalss. The crosstab shows that Person with SibSp>3 were all in Pclass. It is imminenet that all the large families in Pclass3 died. ","metadata":{}},{"cell_type":"markdown","source":"# Parch","metadata":{}},{"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap = 'summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:04.992036Z","iopub.execute_input":"2022-07-14T05:16:04.992476Z","iopub.status.idle":"2022-07-14T05:16:05.021140Z","shell.execute_reply.started":"2022-07-14T05:16:04.992444Z","shell.execute_reply":"2022-07-14T05:16:05.019963Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"The crosstab again shows that larger families were in Pclass3.","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.pointplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:05.022811Z","iopub.execute_input":"2022-07-14T05:16:05.023132Z","iopub.status.idle":"2022-07-14T05:16:05.624877Z","shell.execute_reply.started":"2022-07-14T05:16:05.023105Z","shell.execute_reply":"2022-07-14T05:16:05.623769Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Observations:\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when someone has >4 on the ship.","metadata":{}},{"cell_type":"markdown","source":"# Fare ---> Continous Feature","metadata":{}},{"cell_type":"code","source":"print('Highest Fare was:',data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:05.626618Z","iopub.execute_input":"2022-07-14T05:16:05.627060Z","iopub.status.idle":"2022-07-14T05:16:05.635189Z","shell.execute_reply.started":"2022-07-14T05:16:05.627018Z","shell.execute_reply":"2022-07-14T05:16:05.634325Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"The lowest fare is 0.0. Wow!! a free luxorious ride.","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:05.636488Z","iopub.execute_input":"2022-07-14T05:16:05.637064Z","iopub.status.idle":"2022-07-14T05:16:06.216242Z","shell.execute_reply.started":"2022-07-14T05:16:05.637034Z","shell.execute_reply":"2022-07-14T05:16:06.215109Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning.","metadata":{}},{"cell_type":"markdown","source":"# # **Observation in a Nutshell for all features:**  \n\nSex: The chance of survival for women is high as compared to men.\n\nPclass: There is a visible trend that being a 1st class passenger give you better chances of survival. The survival rate for Pclass3 is very low. For women, the chance of surival from Pclass1 is almost 1 and is high too for those from Pclass2. Money wins!!\n\nAge: Children less than 5-10 years do have a high chance of survival. Passengers between age griuo 15 to 35 died alot.\n\nEmbarked: THis is a very interessting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got on at S. Passengers at Q were all from Pclass3.\n\nParch+SibSp:having 1-2 siblings, spouse on board or 1-3 parents shows a greater chance of porbability rather than being alone or having a large family travelling with you.","metadata":{}},{"cell_type":"markdown","source":"# Correlation Between The Features  \n","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths = 0.2)\nfig=plt.gcf() #gcf()로 현재의 Figure 객체를 구할 수 있다\n# figure을 많이 만들어 놓으면 어떤 figure이 있는지 알기 어려워진다. \n#현재 figure를 확인하기 위한 방법으로는 plt.gcf()를 사용한다. gcf는 get current figure의 약어\n\nfig.set_size_inches(10,8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:06.217718Z","iopub.execute_input":"2022-07-14T05:16:06.218047Z","iopub.status.idle":"2022-07-14T05:16:06.819239Z","shell.execute_reply.started":"2022-07-14T05:16:06.218018Z","shell.execute_reply":"2022-07-14T05:16:06.818374Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**Interpreting the heatmap  **\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.  \nPositive correlation: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.  \nNegative correlation: If an increase in feature A leads to decrease in B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in on leads to increase in the other. This means that both the features are containing hihgly similar information and there is very little or no variance in information.This is known as Multicollinearity as both of them contarins almost the same information. \nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many sich advantages. \nNow from the above heatmap, we can see that the features are not much correlated. The highest correlation is between SibSp and Parch ie 0.41. So we can carry on with all features.  \n\n이제 두 기능이 고도로 또는 완벽하게 상관 관계가 있으므로 on이 증가하면 다른 특성이 증가한다고 가정해 보겠습니다. 이는 두 기능이 모두 매우 유사한 정보를 포함하고 정보의 변동이 거의 또는 전혀 없음을 의미합니다. 이는 둘 다 거의 동일한 정보를 포함하므로 다중 공선성으로 알려져 있습니다. 따라서 둘이 중복되므로 둘 다 사용해야 한다고 생각하십니까? 모델을 만들거나 교육하는 동안 교육 시간과 많은 이점을 줄이므로 중복 기능을 제거하려고 합니다. 이제 위의 히트맵에서 특성이 그다지 상관관계가 없음을 알 수 있습니다. 가장 높은 상관관계는 SibSp와 Parch(즉, 0.41) 사이입니다. 따라서 모든 기능을 계속 사용할 수 있습니다.\n\n**Part2. Feature Engineering and Data Cleaning**\nNow what is Feature Engineering?\n\nWHenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe many redundant features which should be eliminated. Also we can get or add new features by obsering or extracting information from oter features.  \nAn example would be getting the Initials feature using the Name Features. Lets see if we can get any **new features and eliminate a few**. Also we will transform the exisitng relevant features to suitable form for predictive modeling.\n\n\n**Age_band**  \nProblem with Age feature:\nAs I have mentioned earlier that Age is a continuous feature, there is  a **problme with continuous variables** in machine learning models. \nEg: I say to group or arrange sports person by sex, we can easily seperate them by male and female.  \nNow if I say to group them by their age, then how would you do it? if there are 30 people, there maybe 30 values. Now this is the problematic. We nee to convert these continuous values into categorical values by either **Binning or Normalisation.** I will be using binning i.e. group a range of ages into a single bin or assgin them a single value.  \nOkay so the maximum age of a passenger was 80, SO lets divide the range from 0-80 ino 5 bins. So 80/5 = 16. So there are 16 bins of size 16. ","metadata":{}},{"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:06.820741Z","iopub.execute_input":"2022-07-14T05:16:06.821163Z","iopub.status.idle":"2022-07-14T05:16:06.847564Z","shell.execute_reply.started":"2022-07-14T05:16:06.821129Z","shell.execute_reply":"2022-07-14T05:16:06.846262Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:06.849171Z","iopub.execute_input":"2022-07-14T05:16:06.849623Z","iopub.status.idle":"2022-07-14T05:16:06.865540Z","shell.execute_reply.started":"2022-07-14T05:16:06.849583Z","shell.execute_reply":"2022-07-14T05:16:06.864575Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"sns.factorplot('Age_band', 'Survived', data=data, col='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:06.866899Z","iopub.execute_input":"2022-07-14T05:16:06.867903Z","iopub.status.idle":"2022-07-14T05:16:07.703372Z","shell.execute_reply.started":"2022-07-14T05:16:06.867865Z","shell.execute_reply":"2022-07-14T05:16:07.702067Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"True that... the survival rate decreases as the age increases irrespective of the Pclass.\n\n**Family_Size and alone**\n  \nAt this point, we can create a new feature called \"family_size\" and \"alone\" and analyse it. This feature is the summation of Parch and SibSp.It gives us a combined data so that we can check if **survival rate have anything to do with family size** of the passengers. Alone will denote whether a passenger is alone or not.","metadata":{}},{"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.pointplot('Family_Size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.pointplot('Alone','Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:07.704527Z","iopub.execute_input":"2022-07-14T05:16:07.704858Z","iopub.status.idle":"2022-07-14T05:16:08.251723Z","shell.execute_reply.started":"2022-07-14T05:16:07.704831Z","shell.execute_reply":"2022-07-14T05:16:08.250949Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Family_Size = 0 means that the passenger is alone. Clearly, if you are alone or family_size = 0, then chances for survival is very low. For family size >4, the chances decreases too. This also looks to be an important feature for the model. Let's examine this further.","metadata":{}},{"cell_type":"code","source":"sns.factorplot('Alone', 'Survived', data=data, hue='Sex', col = 'Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:08.252871Z","iopub.execute_input":"2022-07-14T05:16:08.253330Z","iopub.status.idle":"2022-07-14T05:16:09.187785Z","shell.execute_reply.started":"2022-07-14T05:16:08.253303Z","shell.execute_reply":"2022-07-14T05:16:09.186869Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"It is visiable that being alone is harmful irrespective of sex or Pclass except for Pclass3 where the survival rate of female who are alone is higher than those with family.\n\n**Fare_Range** \n\nSince fare is also a continuous feature, we need to convert it into **ordinal value** (자연스러운 순번, 서순). For this we will use pandas.qcut.\nSo what qcut does is it splits or arranges the values according the **number of bins** we have passed. So if we pass for 5 bins, it will arrange the values equally spcaed into 5 seperate bins ro value ranges.\n\n","metadata":{}},{"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:09.188944Z","iopub.execute_input":"2022-07-14T05:16:09.189469Z","iopub.status.idle":"2022-07-14T05:16:09.218922Z","shell.execute_reply.started":"2022-07-14T05:16:09.189434Z","shell.execute_reply":"2022-07-14T05:16:09.217862Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"As discussed above, we can clearly see that as the fare_reange increases, the chances of survival increases.\n\nNow we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in Age_Band","metadata":{}},{"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91, 'Fare_cat']=0\ndata.loc[(data['Fare']>7.91) & (data['Fare']<= 14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare']>14.454) & (data['Fare']<=31), 'Fare_cat']=2\ndata.loc[(data['Fare']>31) & (data['Fare']<=513), 'Fare_cat']=3\nsns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:09.220197Z","iopub.execute_input":"2022-07-14T05:16:09.220525Z","iopub.status.idle":"2022-07-14T05:16:09.811138Z","shell.execute_reply.started":"2022-07-14T05:16:09.220488Z","shell.execute_reply":"2022-07-14T05:16:09.810120Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Clearly as the Fare_cat increases the survival chance increases. THis feature may become an important feature during modeling along with the sex.\n\n**Converting String Values into Numeric**\n\nSince we cannot pass strings to a machine learning model, we need to convert features like Sex, Embarked, etc into numeric values.","metadata":{}},{"cell_type":"code","source":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:09.812974Z","iopub.execute_input":"2022-07-14T05:16:09.813313Z","iopub.status.idle":"2022-07-14T05:16:09.825628Z","shell.execute_reply.started":"2022-07-14T05:16:09.813284Z","shell.execute_reply":"2022-07-14T05:16:09.824587Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Dropping Unneeded Features\n\nName --> we don't need name feature as it cannot be convered into any categorical value.\n\nAge --> we have the Age_band feature, so no need of this.\n\nTicket --> it is any random string that cannot be categorised.\n\nFare --> We have the Fare_cat feature, so unneeded\n\nCabin --> alot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\nFare_Range --> we have the fare_cat feature\n\nPassengerId ---> cannot be categorised.","metadata":{}},{"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\n#필요없는 feature들은 버린다\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:09.826701Z","iopub.execute_input":"2022-07-14T05:16:09.827791Z","iopub.status.idle":"2022-07-14T05:16:10.585143Z","shell.execute_reply.started":"2022-07-14T05:16:09.827759Z","shell.execute_reply":"2022-07-14T05:16:10.584016Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Now the above correlation plot, we can see some positively related features. Some of them being SibSp andd **Family_Size and Parch and Family_Size** and some negative ones like **Alone and Family_Size**.","metadata":{}},{"cell_type":"markdown","source":"**Part3: Predictive Modeling**\n\n\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\n\n**EDA (Exploratory Data Analysis) 탐색적 데이터 분석**\n\n수집한 데이터가 들어왔을 때, 이를 다양한 각도에서 관찰하고 이해하는 과정입니다. 한마디로 데이터를 분석하기 전에 그래프나 통계적인 방법으로 자료를 직관적으로 바라보는 과정입니다.\n\n1)Logistic Regression\n\n로지스틱 회귀의 목적은 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. 흔히 로지스틱 회귀는 종속변수가 이항형 문제(즉, 유효한 범주의 개수가 두개인 경우)를 지칭할 때 사용된다. \n\n2)Support Vector Machines(Linear and radial)\n\n기계 학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델이며, 주로 분류와 회귀 분석을 위해 사용한다. 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델을 만든다. \n\n3)Random Forest\n\n분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로, 훈련 과정에서 구성한 다수의 결정 트리로부터 부류(분류) 또는 평균 예측치(회귀 분석)를 출력함으로써 동작한다.\n \n4)K-Nearest Neighbours\n\n분류에서 출력은 소속된 항목이다. 객체는 k개의 최근접 이웃 사이에서 가장 공통적인 항목에 할당되는 객체로 과반수 의결에 의해 분류된다\n\n5)Naive Bayes\n\nnaive bayes classifier같은 경우 모두 conditionally independent하다고 가정한다. 기본적인 bayesian 방법에 임의의 bias를 걸어준 것으로 볼 수 있다. \n\n6)Decision Tree\n\n의사결정나무는 범주나 연속형 수치 모두 예측 가능\n\n7)Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.586502Z","iopub.execute_input":"2022-07-14T05:16:10.587357Z","iopub.status.idle":"2022-07-14T05:16:10.807952Z","shell.execute_reply.started":"2022-07-14T05:16:10.587320Z","shell.execute_reply":"2022-07-14T05:16:10.806590Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]] # 문제지 (train + test)\nY=data['Survived']  # 답안지 (train + test)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.809740Z","iopub.execute_input":"2022-07-14T05:16:10.810043Z","iopub.status.idle":"2022-07-14T05:16:10.823927Z","shell.execute_reply.started":"2022-07-14T05:16:10.810018Z","shell.execute_reply":"2022-07-14T05:16:10.822939Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Radial Support Vector Machines(rbf-SVM)**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.825565Z","iopub.execute_input":"2022-07-14T05:16:10.826271Z","iopub.status.idle":"2022-07-14T05:16:10.856539Z","shell.execute_reply.started":"2022-07-14T05:16:10.826241Z","shell.execute_reply":"2022-07-14T05:16:10.855458Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Linear Support Vector Machine(linear-SVM)","metadata":{}},{"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.857687Z","iopub.execute_input":"2022-07-14T05:16:10.858473Z","iopub.status.idle":"2022-07-14T05:16:10.881672Z","shell.execute_reply.started":"2022-07-14T05:16:10.858440Z","shell.execute_reply":"2022-07-14T05:16:10.880872Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.882737Z","iopub.execute_input":"2022-07-14T05:16:10.883707Z","iopub.status.idle":"2022-07-14T05:16:10.904486Z","shell.execute_reply.started":"2022-07-14T05:16:10.883673Z","shell.execute_reply":"2022-07-14T05:16:10.903509Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"Decision Tree","metadata":{}},{"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.905677Z","iopub.execute_input":"2022-07-14T05:16:10.905955Z","iopub.status.idle":"2022-07-14T05:16:10.919258Z","shell.execute_reply.started":"2022-07-14T05:16:10.905931Z","shell.execute_reply":"2022-07-14T05:16:10.917995Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"K-Nearest Neighbours(KNN)","metadata":{}},{"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.920594Z","iopub.execute_input":"2022-07-14T05:16:10.921685Z","iopub.status.idle":"2022-07-14T05:16:10.943525Z","shell.execute_reply.started":"2022-07-14T05:16:10.921642Z","shell.execute_reply":"2022-07-14T05:16:10.942421Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Now the accuracy for the KNN model changes as we change the values for n_neighbours attribute. The default value is 5. Lets check the accuracies over various values of n_neighbours.","metadata":{}},{"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y))) \n    #metrics.accuracy_score(prediction,test_Y)가 정확도를 반환하여 그 값이  Series의 data parameter로 지정.\n    # Series에서 우리가 원하는 특징값을 index값으로 줄수 있다.\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf() # get current figure: 현제 그림 가져오기\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:10.945184Z","iopub.execute_input":"2022-07-14T05:16:10.946159Z","iopub.status.idle":"2022-07-14T05:16:11.277586Z","shell.execute_reply.started":"2022-07-14T05:16:10.946113Z","shell.execute_reply":"2022-07-14T05:16:11.276311Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:11.278889Z","iopub.execute_input":"2022-07-14T05:16:11.279891Z","iopub.status.idle":"2022-07-14T05:16:11.291070Z","shell.execute_reply.started":"2022-07-14T05:16:11.279857Z","shell.execute_reply":"2022-07-14T05:16:11.289900Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Random Forests","metadata":{}},{"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:11.293544Z","iopub.execute_input":"2022-07-14T05:16:11.295145Z","iopub.status.idle":"2022-07-14T05:16:11.468532Z","shell.execute_reply.started":"2022-07-14T05:16:11.295097Z","shell.execute_reply":"2022-07-14T05:16:11.467160Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of a model is not the only factor that determines the robustness(견고함) of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as **model variance**.\n\nTo overcome this and get a generalized model,we use **Cross Validation**.\n\n\n**Cross Validation**\n교차검증\n\nMany a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n\n1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n\n2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n\n3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\nThis is called K-Fold Cross Validation.\n\n4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22, shuffle = True) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:11.469761Z","iopub.execute_input":"2022-07-14T05:16:11.470105Z","iopub.status.idle":"2022-07-14T05:16:14.000560Z","shell.execute_reply.started":"2022-07-14T05:16:11.470064Z","shell.execute_reply":"2022-07-14T05:16:13.999461Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:14.001698Z","iopub.execute_input":"2022-07-14T05:16:14.002155Z","iopub.status.idle":"2022-07-14T05:16:14.234477Z","shell.execute_reply.started":"2022-07-14T05:16:14.002126Z","shell.execute_reply":"2022-07-14T05:16:14.233349Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:14.235680Z","iopub.execute_input":"2022-07-14T05:16:14.235973Z","iopub.status.idle":"2022-07-14T05:16:14.438553Z","shell.execute_reply.started":"2022-07-14T05:16:14.235947Z","shell.execute_reply":"2022-07-14T05:16:14.437427Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.","metadata":{}},{"cell_type":"markdown","source":"# **Confusion Matrix**\nIt gives the number of correct and incorrect classifications made by the classifier.","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace = 0.2, wspace = 0.2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:14.439733Z","iopub.execute_input":"2022-07-14T05:16:14.440037Z","iopub.status.idle":"2022-07-14T05:16:19.197550Z","shell.execute_reply.started":"2022-07-14T05:16:14.440009Z","shell.execute_reply":"2022-07-14T05:16:19.196397Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# **Interpreting Confusion Matrix**\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\n\n1)The no. of correct predictions are 491(for dead) + 247(for survived) with the mean CV accuracy being (491+247)/891 = 82.8% which we did get earlier.\n\n2)Errors--> Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n58명의 사망자가 생존자로, 95명이 사망한 것으로 잘못 분류되었습니다. 따라서 죽은 사람을 생존자로 예측함으로써 더 많은 실수를 저질렀습니다.\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.\n\n**Hyper-Parameters Tuning**\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\n\nWe will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.\n\n**SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n#GridSearchCV란 사이킷런에서 분류 알고리즘이나 회귀 알고리즘에 사용되는 하이퍼파라미터를 순차적으로 입력해 학습을 하고 측정을 하면서 \n#가장 좋은 파라미터를 알려준다. GridSearchCV가 어떤 parameter 값이 최적인 스코어를  뽑아내는지 일일이 학습을 해야 한다. \n#하지만 grid 파라미터 안에서 집합을 만들고 적용하면 최적화된 파라미터를 뽑아낼 수 있다.\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:19.199092Z","iopub.execute_input":"2022-07-14T05:16:19.200438Z","iopub.status.idle":"2022-07-14T05:16:48.580988Z","shell.execute_reply.started":"2022-07-14T05:16:19.200385Z","shell.execute_reply":"2022-07-14T05:16:48.579604Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"**Random Forests**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:16:48.582465Z","iopub.execute_input":"2022-07-14T05:16:48.583375Z","iopub.status.idle":"2022-07-14T05:17:25.050284Z","shell.execute_reply.started":"2022-07-14T05:16:48.583328Z","shell.execute_reply":"2022-07-14T05:17:25.049135Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Ensembling\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stability of the model. Ensembling can be done in ways like:\n\n1)Voting Classifier\n\n2)Bagging\n\n3)Boosting.","metadata":{}},{"cell_type":"markdown","source":"**Voting Classifier**\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of different types. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:17:25.051574Z","iopub.execute_input":"2022-07-14T05:17:25.051978Z","iopub.status.idle":"2022-07-14T05:17:36.684011Z","shell.execute_reply.started":"2022-07-14T05:17:25.051946Z","shell.execute_reply":"2022-07-14T05:17:36.682672Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# # **Bagging**\n\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n\nBagging은 입력 데이터를 모델 수 만큼 나눈 뒤, 각각 학습시킨다.\n이후, Test dataset 을 각 모델에 넣어 예측할 때, 출력되어 나온 예측 값들을 voting 하여, 보다 더 투표를 받은 예측 값이 최종 예측값이 된다.\n\n\n**Bagged KNN**\n\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours.\n\n배깅은 분산이 높은 모델에서 가장 잘 작동합니다. 이에 대한 예로는 의사 결정 트리 또는 랜덤 포레스트가 있습니다. KNN의 n_neighbours 값을 n_neighbours의 작은 값으로 사용해 보도록 하겠습니다.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:17:36.687553Z","iopub.execute_input":"2022-07-14T05:17:36.687912Z","iopub.status.idle":"2022-07-14T05:17:58.670934Z","shell.execute_reply.started":"2022-07-14T05:17:36.687880Z","shell.execute_reply":"2022-07-14T05:17:58.669697Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# ****Boosting  \n\n\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n\n부스팅은 분류기의 순차적인 학습을 이용한 앙상블 기법입니다. 순차적으로 약한 모델을 향상시켜나갑니다.\n부스팅은 아래와 같이 작동합니다 :\n\n모델은 처음 전체 데이터셋에 대해 학습합니다. 이 때 모델은 일부 객체는 올바르게, 일부 객체는 틀리게 예측할 것입니다.\n그 다음 시행에서, 틀리게 예측한 객체에 더욱 가중치를 두어 학습합니다. 결과적으로 틀리게 예측한 객체를 올바르게 예측하려고 노력합니다.\n이런 과정이 반복되면서, 정확도가 한계에 도달할 때까지 새 분류기가 모델에 추가됩니다.\n\n**AdaBoost(Adaptive Boosting)**\n\nThe weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base_estimator to any algorithm of our choice.\n\n다른 학습 알고리즘(약한 학습기, weak learner)의 결과물들을 가중치를 두어 더하는 방법으로 가속화 분류기의 최종 결과물을 표현할 수 있다. AdaBoost는 이전의 분류기에 의해 잘못 분류된 것들을 이어지는 약한 학습기들이 수정해줄 수 있다는 점에서 다양한 상황에 적용할 수 있다(adaptive).","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:17:58.678401Z","iopub.execute_input":"2022-07-14T05:17:58.678752Z","iopub.status.idle":"2022-07-14T05:18:02.062141Z","shell.execute_reply.started":"2022-07-14T05:17:58.678725Z","shell.execute_reply":"2022-07-14T05:18:02.060923Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"**Stochastic Gradient Boosting**\nHere too the weak learner is a Decision Tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:18:02.063557Z","iopub.execute_input":"2022-07-14T05:18:02.063893Z","iopub.status.idle":"2022-07-14T05:18:06.454567Z","shell.execute_reply.started":"2022-07-14T05:18:02.063862Z","shell.execute_reply":"2022-07-14T05:18:06.453502Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:18:06.455899Z","iopub.execute_input":"2022-07-14T05:18:06.456330Z","iopub.status.idle":"2022-07-14T05:18:32.787054Z","shell.execute_reply.started":"2022-07-14T05:18:06.456295Z","shell.execute_reply":"2022-07-14T05:18:32.785925Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"We got the highest accuracy for AdaBoost. We will try to increase it with Hyper-Parameter Tuning\n\nAdaBoost가 가장 높은 정확도를 기록했습니다. 이 정확도를 하이퍼파라미터 튜닝을 통해 더 높여보겠습니다.\n\n\n**Hyper-Parameter Tuning for AdaBoost**","metadata":{}},{"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_) # Mean cross-validated score of the best_estimator\nprint(gd.best_estimator_) #Estimator that was chosen by the search, \n#i.e. estimator which gave highest score (or smallest loss if specified) on the left out data","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:18:32.788525Z","iopub.execute_input":"2022-07-14T05:18:32.788832Z","iopub.status.idle":"2022-07-14T05:27:32.782269Z","shell.execute_reply.started":"2022-07-14T05:18:32.788806Z","shell.execute_reply":"2022-07-14T05:27:32.781100Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# # # **Confusion Matrix for the Best Model**\n","metadata":{}},{"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:27:32.783479Z","iopub.execute_input":"2022-07-14T05:27:32.783790Z","iopub.status.idle":"2022-07-14T05:27:36.353386Z","shell.execute_reply.started":"2022-07-14T05:27:32.783763Z","shell.execute_reply":"2022-07-14T05:27:36.352135Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"**Feature Importance**","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-14T05:27:36.354810Z","iopub.execute_input":"2022-07-14T05:27:36.355245Z","iopub.status.idle":"2022-07-14T05:27:41.053066Z","shell.execute_reply.started":"2022-07-14T05:27:36.355214Z","shell.execute_reply":"2022-07-14T05:27:41.051838Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\n\n**Observations:**\n\n1)Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\n\n2)The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\n\nHowever, we can see the feature Initial, which is at the top in many classifiers.We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n\n3)Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp.\n\n\n1) 공통적으로 중요한 feature는 Initial ,Fare_cat, Pclass, Family_Size 입니다.\n\n2) Sex 는 그렇게 중요도가 높지 않았는데, 앞선 분석에서 Pclass와 함께 보았을 때 성별이 중요한 요소였던 것을 생각하면 놀라운 결과입니다.\n성별은 Random Forest 모델에서만 중요해보입니다.\n\n하지만 많은 분류기의 최상단에 있는 Initial은 Sex과 양의 상관관계에 있습니다. 결국, 두 정보 모두 성별에 대한 정보를 담고 있습니다.\n\n3) 이와 비슷하게 Pclass와 Fare_cat 모두 탑승객의 지위와 Family_Size, Alone, Parch, SibSp의 정보를 담고 있습니다.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}